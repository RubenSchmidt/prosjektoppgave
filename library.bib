Automatically generated by Mendeley Desktop 1.17.12
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@misc{Collobert2017,
author = {Collobert, Ronan and Farabet, Clement and Kavukcuoglu, Koray and Chintala, Soumith},
title = {{Torch}},
url = {http://torch.ch/},
urldate = {2017-12-19},
year = {2017}
}
@misc{BerkeleyAIResearchBAIR2017,
author = {{Berkeley AI Research (BAIR)}},
title = {{Caffe}},
url = {http://caffe.berkeleyvision.org/},
urldate = {2017-12-19},
year = {2017}
}
@article{Chiang2013,
abstract = {Raster maps are easily accessible and contain rich road information; however, converting the road information to vector format is challenging because of varying image quality, overlapping features, and typical lack of metadata (e.g., map geocoordinates). Previous road vectorization approaches for raster maps typically handle a specific map series and require significant user effort. In this paper, we present a general road vectorization approach that exploits common geometric properties of roads in maps for processing heterogeneous raster maps while requiring minimal user intervention. In our experiments, we compared our approach to a widely used commercial product using 40 raster maps from 11 sources. We showed that overall our approach generated high-quality results with low redundancy with considerably less user input compared with competing approaches.},
author = {Chiang, Yao Yi and Knoblock, Craig A.},
doi = {10.1007/s10032-011-0177-1},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/Chiang, Knoblock - 2013 - A general approach for extracting road vector data from raster maps.pdf:pdf},
isbn = {9783642330230},
issn = {14332833},
journal = {International Journal on Document Analysis and Recognition},
keywords = {GIS,Map processing,Raster maps,Road vectorization},
number = {1},
pages = {55--81},
title = {{A general approach for extracting road vector data from raster maps}},
volume = {16},
year = {2013}
}
@article{Pearson2006,
annote = {Kilde p{\aa} hvorfor digitalisering av gamle kart kan v{\ae}re verdifult},
author = {Pearson, Alastair W},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/Pearson.pdf:pdf},
keywords = {agricultural history,georeferencing,gis,multilevel modelling,tithe maps},
number = {3},
pages = {178--193},
title = {{Digitizing and analyzing historical maps to provide new perspectives on the development of the agricultural landscape of England and Wales}},
volume = {1},
year = {2006}
}
@article{Schalkoff1989,
author = {Schalkoff, R J},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/Schalkoff - 1989 - Feature Extraction And Shape Classification Of 2-D Polygons Using A Neural Network.pdf:pdf},
pages = {953--958},
title = {{Feature Extraction And Shape Classification Of 2-D Polygons Using A Neural Network}},
year = {1989}
}
@article{Iosifescu2016,
abstract = {Historical maps from different periods of time are very important for many types of research. They can show the development of a place through the time and their use can be profitable in different studies concerning the geographic analysis of terrain, en- vironmental changes and the development of landscape and settlements in a specific area. These spatial changes are many times preserved only through maps that are often available only in analogue form or, in the best case, as scanned raster images. Since the scanning of these maps is not always sufficient for their further analysis, it is useful and practical to have historical maps in vector form and the most important, to have a method to automati- cally convert the raster historical maps to vector data. The extracted vector data gives re- searchers and historians the opportunity to detect and determine more easily spatial chang- es in an area over time and also makes easier the combination and the analysis of historical and modern data in order to highlight in a faster manner the differences between maps dated on various time periods.},
author = {Iosifescu, Ionut and Tsorlini, Angeliki and Hurni, Lorenz},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/Iosifescu, Tsorlini, Hurni - 2016 - Towards a comprehensive methodology for automatic vectorization of raster historical maps.pdf:pdf},
journal = {e-Perimetron},
keywords = {automatic vectorization,historical maps,raster to vector,shape recognition,vectorization algorithms},
number = {2},
pages = {57--76},
title = {{Towards a comprehensive methodology for automatic vectorization of raster historical maps}},
volume = {11},
year = {2016}
}
@misc{powertrace2016,
author = {PowerTRACE},
title = {{Taking Corel PowerTRACE for a Test Drive – Knowledge Base}},
url = {https://support.corel.com/hc/en-us/articles/215943948-Taking-Corel-PowerTRACE-for-a-Test-Drive},
urldate = {2017-10-24},
year = {2016}
}
@misc{scan2cad2009,
author = {Scan2cad},
title = {{Scan2CAD in Landscape Architecture}},
url = {https://www.scan2cad.com/user-testimony/scan2cad-in-landscape-architecture/},
urldate = {2017-10-24},
year = {2009}
}
@article{Karabork2008,
abstract = {In the last twenty-five years a great number of vectorization methods were developed. In this paper we first give an overview about the most known methods, and then propose a vectorization algorithm based on Artificial Neural Network method. This algorithm is implemented by using C{\#} programming language. Because distortions in size and location after vectorization are important for mapping applications, we tested our algorithm and software on a cadastral map. We also compared the results of our algorithm with the results of Sparse Pixel Vectorization (SPV) algorithm. Although SPV algorithm delivers better results, our algorithm also givesacceptable results, which are suitable for mapping purposes.},
author = {Karabork, H and Kocer, B. and Bildirici, I O and Yildiz, F. and Aktas, E.},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/Karabork et al. - 2008 - A neural network algorithm for vectorization of 2D maps.pdf:pdf},
journal = {[APRS'08] International Archives of Photogrammetry and Remote Sensing},
keywords = {accuracy,analysis,artificial{\_}intelligence,cad,image,information,spatial information sciences,tracking},
number = {B2},
pages = {473--480},
title = {{A neural network algorithm for vectorization of 2D maps}},
volume = {XXXVII},
year = {2008}
}
@article{Armstrong2006,
author = {Armstrong, Curtis A},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/Armstrong - 2006 - Vectorization of Raster Images Using B-Spline Surfaces.pdf:pdf},
keywords = {vector video research preparation,矢量视频调研},
number = {July},
pages = {166},
title = {{Vectorization of Raster Images Using B-Spline Surfaces}},
year = {2006}
}
@article{Daniil2003,
abstract = {The introduction of modern digital imaging techniques to historical cartography, especially in documenting old map collections, meets the need for extending and applying image processing and relevant photogrammetric tools to the analysis of antique maps in a context in which digital photogrammetry intersects modern theories of digital treatment of old maps. The authors discuss some aspects of controlling the geometric features of the scanned images of antique maps in terms, e.g., of linear, angular, and areal deformation. Collateral effects influencing the scanned map image are also described--namely, the statistics and the image quality of the output map images with respect to the original map and/or the "reference" digital best-scanned image.},
author = {Daniil, M and Tsioukas, V and Papadopoulos, K and Livieratos, E},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/Scanning{\_}options{\_}and{\_}choices{\_}in{\_}digitizing{\_}histori.pdf:pdf},
isbn = {975-561-245-9},
journal = {Proceedings of the XIXth International Symposium, CIPA 2003: new perspectives to save cultural heritage: Antalya (Turkey), 30 September-04 October, 2003},
keywords = {cartography,data processing,digital imaging,maps,photogrammetry},
number = {January},
pages = {99--102},
title = {{Scanning options and choices in digitizing historic maps}},
year = {2003}
}
@article{Oka2012,
abstract = {In this paper we propose a general method for contour recognition and DEM (Digital Elevation Models) generation from commercially available printed topographic maps. Automatic contour recognition of a scanned topographic map is a difficult problem due to the presence of closely spaced or broken contours, overlapping data layers and complexities arising from textured background etc. Beginning with a scanned map our approach utilizes a multistage process which broadly includes contour identification, cleaning and vectorization. While the identification and vectorization steps are based on calibrating existing image processing techniques, the cleaning step uses a modified geodesic distance based approach. The proposed technique was tested on contour regions of varying complexity scanned from a Survey of India toposheet of scale 1:50,000, scanned at 300 DPI. It was found that the proposed technique resulted in clean segmentation, crisp contour borders in the case of regular contour regions. However, differing levels of manual interventions were required to vectorize complex contour regions represented in the printed map. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
annote = {De klarer ganske bra {\aa} hente ut kontur linjer fra kart. Men tester bare en type kart.},
author = {Oka, Shriram and Garg, Akash and Varghese, Koshy},
doi = {10.1016/j.autcon.2011.06.017},
file = {:C$\backslash$:/Users/Ruben/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Oka, Garg, Varghese - 2012 - Vectorization of contour lines from scanned topographic maps.pdf:pdf},
isbn = {09265805 (ISSN)},
issn = {09265805},
journal = {Automation in Construction},
keywords = {Automated contour recognition,Contour maps,Digital elevation models,Geodesic distance,Topographic maps},
pages = {192--202},
publisher = {Elsevier B.V.},
title = {{Vectorization of contour lines from scanned topographic maps}},
url = {http://dx.doi.org/10.1016/j.autcon.2011.06.017},
volume = {22},
year = {2012}
}
@article{Visin2015,
abstract = {In this paper, we propose a deep neural network architecture for object recognition based on recurrent neural networks. The proposed network, called ReNet, replaces the ubiquitous convolution+pooling layer of the deep convolutional neural network with four recurrent neural networks that sweep horizontally and vertically in both directions across the image. We evaluate the proposed ReNet on three widely-used benchmark datasets; MNIST, CIFAR-10 and SVHN. The result suggests that ReNet is a viable alternative to the deep convolutional neural network, and that further investigation is needed.},
archivePrefix = {arXiv},
arxivId = {1505.00393},
author = {Visin, Francesco and Kastner, Kyle and Cho, Kyunghyun and Matteucci, Matteo and Courville, Aaron and Bengio, Yoshua},
doi = {10.1109/CVPR.2016.399},
eprint = {1505.00393},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/1505.00393.pdf:pdf},
isbn = {9781577357384},
issn = {10450823},
pages = {1--9},
title = {{ReNet: A Recurrent Neural Network Based Alternative to Convolutional Networks}},
url = {http://arxiv.org/abs/1505.00393},
year = {2015}
}
@misc{AmazonMechanicalTurk2017,
author = {{Amazon Mechanical Turk}},
title = {{Amazon Mechanical Turk (MTurk)}},
url = {https://www.mturk.com/},
urldate = {2017-12-03},
year = {2017}
}
@misc{Kommunaltplanregister2009,
author = {{Kommunalt Planregister}},
title = {{{\S} 2-2. Kommunalt planregister}},
url = {https://www.regjeringen.no/no/dokument/dep/kmd/veiledninger{\_}brosjyrer/2009/lovkommentar-til-plandelen-i-/kapittel-2-krav-om-kartgrunnlag-stedfest/-2-2-kommunalt-planregister/id556741/},
year = {2009}
}
@article{Long2014,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20{\%} relative improvement to 62.2{\%} mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
doi = {10.1109/TPAMI.2016.2572683},
eprint = {1411.4038},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/1411.4038.pdf:pdf},
isbn = {978-1-4673-6964-0},
issn = {01628828},
pmid = {16190471},
title = {{Fully Convolutional Networks for Semantic Segmentation}},
url = {http://arxiv.org/abs/1411.4038},
year = {2014}
}
@article{Yu2015,
abstract = {State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy.},
archivePrefix = {arXiv},
arxivId = {1511.07122},
author = {Yu, Fisher and Koltun, Vladlen},
doi = {10.16373/j.cnki.ahr.150049},
eprint = {1511.07122},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/1511.07122.pdf:pdf},
isbn = {0894-0282},
issn = {00237205},
pmid = {22352717},
title = {{Multi-Scale Context Aggregation by Dilated Convolutions}},
url = {http://arxiv.org/abs/1511.07122},
year = {2015}
}
@misc{Tensorflow2017,
author = {Tensorflow},
title = {{Tensorflow}},
url = {https://www.tensorflow.org/},
urldate = {2017-12-19},
year = {2017}
}
@misc{Microsoft2017,
author = {Microsoft},
title = {{The Microsoft Cognitive Toolkit}},
url = {https://docs.microsoft.com/en-us/cognitive-toolkit/},
urldate = {2017-12-19},
year = {2017}
}
@article{Krahenbuhl2012a,
abstract = {Most state-of-the-art techniques for multi-class image segmentation and labeling use conditional random fields defined over pixels or image regions. While region-level models often feature dense pairwise connectivity, pixel-level models are considerably larger and have only permitted sparse graph structures. In this paper, we consider fully connected CRF models defined on the complete set of pixels in an image. The resulting graphs have billions of edges, making traditional inference algorithms impractical. Our main contribution is a highly efficient approximate inference algorithm for fully connected CRF models in which the pairwise edge potentials are defined by a linear combination of Gaussian kernels. Our experiments demonstrate that dense connectivity at the pixel level substantially improves segmentation and labeling accuracy.},
archivePrefix = {arXiv},
arxivId = {1210.5644},
author = {Kr{\"{a}}henb{\"{u}}hl, Philipp and Koltun, Vladlen},
eprint = {1210.5644},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/densecrf.pdf:pdf},
isbn = {9781618395993},
issn = {9781618395993},
pages = {1--9},
title = {{Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials}},
url = {http://arxiv.org/abs/1210.5644},
year = {2012}
}
@article{Zangeneh2011,
abstract = {Potatoes are the single most important agricultural commodity in Hamadan province of Iran, where 25,503 ha of this crop were planted in 2008 under irrigated conditions. This paper compares results of the application of two different approaches, parametric model (PM) and artificial neural networks (ANNs), for assessing economical productivity (EP), total costs of production (TCP) and benefit to cost ratio (BC) of potato crop. In this comparison, Cobb-Douglas function for PM and multilayer feedforward for implementing ANN models have been used. The ANN, having 8-6-12-1 topology with R 2 = 0.89, resulted in the best-suited model for estimating EP. Similarly, optimal topologies for TCP and BC were 8-13-15-1 (R 2 = 0.97) and 8-15-13-1 (R 2 = 0.94), respectively. In validating the PM and ANN models, mean absolute percentage error (MAPE) was used as performance indicator. The ANN approach allowed to reduce the MAPE from –184{\%} for PM to less than 7{\%} with a +30{\%} to –95{\%} variability range. Since ANN outperformed PM model, it should be preferred for estimating economical indices. Resumen Estudio comparativo entre enfoques param{\'{e}}tricos y de redes neuronales artificiales para la evaluaci{\'{o}}n econ{\'{o}}mica de la producci{\'{o}}n de patata en Ir{\'{a}}n La patata es el producto agr{\'{i}}cola m{\'{a}}s importante en la provincia de Hamadan (Ir{\'{a}}n), donde se plantaron 25.503 ha de este cultivo en 2008 bajo condiciones de riego. Este trabajo compara los resultados de aplicar dos enfoques dife-rentes, un modelo param{\'{e}}trico (PM) y redes neuronales artificiales (ANN), para evaluar la productividad econ{\'{o}}mica (EP), los costos totales de producci{\'{o}}n (TCP) y el coeficiente beneficio/costo (BC) del cultivo de la patata. En esta comparaci{\'{o}}n se han utilizado la funci{\'{o}}n Cobb-Douglas como PM y el proceso " feedforward " multicapa para imple-mentar modelos de ANN. Las ANN, con una topolog{\'{i}}a 8-6-12-1 con R 2 = 0,89, resultaron ser el modelo m{\'{a}}s adecua-do para estimar la EP. Del mismo modo, las topolog{\'{i}}as {\'{o}}ptimas para TCP y BC fueron 8-13-15-1 (R 2 = 0,97) y 8-15-13-1 (R 2 = 0,94), respectivamente. Para validar los modelos PM y ANN, se utiliz{\'{o}} como indicador de desempe{\~{n}}o el error porcentual medio absoluto (MAPE). El enfoque de ANN permiti{\'{o}} reducir el MAPE desde –184{\%} para PM a me-nos del 7{\%} con un rango de variabilidad de +30{\%} a–95{\%}. Dado que ANN fue mejor que el modelo PM, debe ser pre-ferido para la estimaci{\'{o}}n de los {\'{i}}ndices econ{\'{o}}micos. Palabras clave adicionales: coeficiente beneficio/costo; costo total de producci{\'{o}}n; error de estimaci{\'{o}}n; funci{\'{o}}n de producci{\'{o}}n Cobb-Douglas; productividad econ{\'{o}}mica; redes neuronales artificiales; Solanum tuberosum. Abbreviations used: ANN (artificial neural network); BC (benefit to cost ratio); CER (cost estimation relationship); EP (econo-mical productivity, kg {\$} –1); MAE (mean absolute error); MAPE (mean absolute percentage error); MLP (multi layer perceptron); MSE (mean squared error); PM (parametric model); TCP (total cost of production, {\$} ha –1).},
annote = {Vectorisering uten masinl{\ae}ring},
author = {Zangeneh, M and Omid, M and Akram, A},
doi = {10.5424/http://dx.doi.org/10.5424/sjar/20110903-371-10},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/Zangenehetal.pdf:pdf},
isbn = {1695-971X},
issn = {2171-9292},
journal = {Instituto Nacional de Investigaci{\'{o}}n y Tecnolog{\'{i}}a Agraria y Alimentaria (INIA) Spanish Journal of Agricultural Research},
keywords = {Additional key words,Cobb-Douglas production function,Solanum tuberosum,artificial neural networks,benefit to cost ratio,eco-nomical productivity,estimation error,total cost of production},
number = {3},
pages = {661--671},
title = {{A comparative study between parametric and artificial neural networks approaches for economical assessment of potato production in Iran}},
url = {www.inia.es/sjar},
volume = {9},
year = {2011}
}
@misc{ImageMagickStudioLLC2017,
author = {{ImageMagick Studio LLC}},
title = {{ImageMagic}},
url = {http://www.imagemagick.org/script/index.php},
urldate = {2017-11-12},
year = {2017}
}
@article{Chen2017,
abstract = {In histopathological image analysis, the morphology of histological structures, such as glands and nuclei, has been routinely adopted by pathologists to assess the malignancy degree of adenocarcinomas. Accurate detection and segmentation of these objects of interest from histology images is an essential prerequisite to obtain reliable morphological statistics for quantitative diagnosis. While manual annotation is error-prone, time-consuming and operator-dependant, automated detection and segmentation of objects of interest from histology images can be very challenging due to the large appearance variation, existence of strong mimics, and serious degeneration of histological structures. In order to meet these challenges, we propose a novel deep contour-aware network (DCAN) under a unified multi-task learning framework for more accurate detection and segmentation. In the proposed network, multi-level contextual features are explored based on an end-to-end fully convolutional network (FCN) to deal with the large appearance variation. We further propose to employ an auxiliary supervision mechanism to overcome the problem of vanishing gradients when training such a deep network. More importantly, our network can not only output accurate probability maps of histological objects, but also depict clear contours simultaneously for separating clustered object instances, which further boosts the segmentation performance. Our method ranked the first in two histological object segmentation challenges, including 2015 MICCAI Gland Segmentation Challenge and 2015 MICCAI Nuclei Segmentation Challenge. Extensive experiments on these two challenging datasets demonstrate the superior performance of our method, surpassing all the other methods by a significant margin.},
archivePrefix = {arXiv},
arxivId = {1604.02677},
author = {Chen, Hao and Qi, Xiaojuan and Yu, Lequan and Dou, Qi and Qin, Jing and Heng, Pheng Ann},
doi = {10.1016/j.media.2016.11.004},
eprint = {1604.02677},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/histology-instace-segmentation.pdf:pdf},
isbn = {9781467388511},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Deep contour-aware network,Deep learning,Histopathological image analysis,Instance segmentation,Object detection,Transfer learning},
pages = {135--146},
pmid = {27898306},
publisher = {Elsevier B.V.},
title = {{DCAN: Deep contour-aware networks for object instance segmentation from histology images}},
url = {http://dx.doi.org/10.1016/j.media.2016.11.004},
volume = {36},
year = {2017}
}
@book{Worboys2003,
abstract = {GIS: A Computing Perspective, Second Edition, provides a full, up-to-date overview of the state-of-the-art in GIS, both Geographic Information Systems and the study of these systems-Geographic Information Science. Analyzing the subject from a computing perspective, the second edition explores conceptual and formal models needed to understand spatial information, and examines the representations and data structures needed to support adequate system performance. This volume also covers the special-purpose interfaces and architectures required to interact with and share spatial information, and explains the importance of uncertainty and time. The material on GIS architectures and interfaces as well as spatiotemporal information systems is almost entirely new. The second edition contains substantial new information, and has been completely reformatted to improve accessibility. Changes include: There is also a new chapter on spatial uncertaintyComplete revisions of the bibliography, index, and supporting diagramsSupplemental material is offset at the top of the page, as are references and links for further studyDefinitions of new terms are in the margins of pages where they appear, with corresponding entries in the index},
author = {Worboys, Michael F.},
booktitle = {CRC press},
isbn = {0-7484-0065-6},
issn = {0028-8144},
pages = {376},
pmid = {41376455},
title = {{GIS: A computing perspective}},
year = {2003}
}
@article{Henderson,
author = {Henderson, Thomas C and Linton, Trevor and Potupchik, Sergey and Ostanin, Andrei},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/Automatic{\_}segmentation{\_}of{\_}semanticclasses{\_}in{\_}raster{\_}map{\_}images.pdf:pdf},
keywords = {graphics recognition,raster map images,segmentation},
title = {{Automatic Segmentation of Semantic Classes in Raster Map Images}}
}
@misc{PASCALVOC2012a,
author = {{PASCAL VOC}},
title = {{Segmentation Results: VOC2012}},
url = {http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11{\&}compid=6{\#}KEY{\_}SegNet},
urldate = {2017-11-12},
year = {2012}
}
@article{Kaiser2017,
abstract = {This study deals with semantic segmentation of high-resolution (aerial) images where a semantic class label is assigned to each pixel via supervised classification as a basis for automatic map generation. Recently, deep convolutional neural networks (CNNs) have shown impressive performance and have quickly become the de-facto standard for semantic segmentation, with the added benefit that task-specific feature design is no longer necessary. However, a major downside of deep learning methods is that they are extremely data-hungry, thus aggravating the perennial bottleneck of supervised classification, to obtain enough annotated training data. On the other hand, it has been observed that they are rather robust against noise in the training labels. This opens up the intriguing possibility to avoid annotating huge amounts of training data, and instead train the classifier from existing legacy data or crowd-sourced maps which can exhibit high levels of noise. The question addressed in this paper is: can training with large-scale, publicly available labels replace a substantial part of the manual labeling effort and still achieve sufficient performance? Such data will inevitably contain a significant portion of errors, but in return virtually unlimited quantities of it are available in larger parts of the world. We adapt a state-of-the-art CNN architecture for semantic segmentation of buildings and roads in aerial images, and compare its performance when using different training data sets, ranging from manually labeled, pixel-accurate ground truth of the same city to automatic training data derived from OpenStreetMap data from distant locations. We report our results that indicate that satisfying performance can be obtained with significantly less manual annotation effort, by exploiting noisy large-scale training data.},
archivePrefix = {arXiv},
arxivId = {1707.06879},
author = {Kaiser, Pascal and Wegner, Jan Dirk and Lucchi, Aurelien and Jaggi, Martin and Hofmann, Thomas and Schindler, Konrad},
doi = {10.1109/TGRS.2017.2719738},
eprint = {1707.06879},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/1707.06879.pdf:pdf},
issn = {01962892},
journal = {IEEE Transactions on Geoscience and Remote Sensing},
keywords = {Crowdsourcing,Image segmentation,Manuals,Roads,Semantics,Training,Training data,Urban areas,image classification,machine learning,neural networks,supervised learning,terrain mapping,urban areas},
pages = {1--15},
title = {{Learning Aerial Image Segmentation From Online Maps}},
year = {2017}
}
@article{Pearson2006,
annote = {Kilde p{\aa} hvorfor digitalisering av gamle kart kan v{\ae}re verdifult},
author = {Pearson, Alastair W},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/Pearson.pdf:pdf},
keywords = {agricultural history,georeferencing,gis,multilevel modelling,tithe maps},
number = {3},
pages = {178--193},
title = {{Digitizing and analyzing historical maps to provide new perspectives on the development of the agricultural landscape of England and Wales}},
volume = {1},
year = {2006}
}
@misc{TheRFoundation2017,
author = {{The R Foundation}},
title = {{R}},
url = {https://www.r-project.org/},
urldate = {2017-11-12},
year = {2017}
}
@article{Hinton2015,
abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
archivePrefix = {arXiv},
arxivId = {1503.02531},
author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
doi = {10.1063/1.4931082},
eprint = {1503.02531},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/1503.02531.pdf:pdf},
isbn = {3531207857},
issn = {0022-2488},
pages = {1--9},
pmid = {18249735},
title = {{Distilling the Knowledge in a Neural Network}},
url = {http://arxiv.org/abs/1503.02531},
year = {2015}
}
@article{Chen2014,
abstract = {Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called "semantic image segmentation"). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our "DeepLab" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6{\%} IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.},
archivePrefix = {arXiv},
arxivId = {1412.7062},
author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
doi = {10.1109/TPAMI.2017.2699184},
eprint = {1412.7062},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/1412.7062.pdf:pdf},
isbn = {9783901608353},
issn = {0162-8828},
pages = {1--14},
pmid = {28463186},
title = {{Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs}},
url = {http://arxiv.org/abs/1412.7062},
year = {2014}
}
@article{Song2000,
author = {Song, Jiqiang and Su, Feng and Chen, Jibing and Cai, Shijie},
doi = {10.1007/s100440070019},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/10.1007{\%}2Fs100440070019.pdf:pdf},
issn = {1433-7541},
journal = {Pattern Analysis {\&} Applications},
keywords = {global vectorisation,knowledge,line network,seed segment,tracking,vectorisation},
number = {2},
pages = {142--152},
title = {{A Knowledge-Aided Line Network Oriented Vectorisation Method for Engineering Drawings}},
volume = {3},
year = {2000}
}
@article{Monagan1993,
abstract = {In order to interpret line drawings accurately, it is essential to$\backslash$nhave a good base representation of the raster image. The authors$\backslash$nadvocate the run graph representation as an appropriate base$\backslash$nrepresentation after scanning and before segmentation: it is information$\backslash$npreserving and it can be used efficiently in line extraction. They give$\backslash$na full definition of run graphs by taking further the work of S. D. Di$\backslash$nZenzo and A. Morelli (1989) and of L. Boatto et al. (1992). The authors$\backslash$npresent a modification needed to use the run graphs effectively in an$\backslash$nindustrial application and conclude with a brief discussion of the$\backslash$nimplementation},
author = {Monagan, G. and Roosli, M.},
doi = {10.1109/ICDAR.1993.395659},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/00395659.pdf:pdf},
isbn = {0-8186-4960-7},
journal = {Proceedings of 2nd International Conference on Document Analysis and Recognition (ICDAR '93)},
pages = {623--626},
title = {{Appropriate base representation using a run graph}},
year = {1993}
}
@article{Iosifescu2016,
abstract = {Historical maps from different periods of time are very important for many types of research. They can show the development of a place through the time and their use can be profitable in different studies concerning the geographic analysis of terrain, en- vironmental changes and the development of landscape and settlements in a specific area. These spatial changes are many times preserved only through maps that are often available only in analogue form or, in the best case, as scanned raster images. Since the scanning of these maps is not always sufficient for their further analysis, it is useful and practical to have historical maps in vector form and the most important, to have a method to automati- cally convert the raster historical maps to vector data. The extracted vector data gives re- searchers and historians the opportunity to detect and determine more easily spatial chang- es in an area over time and also makes easier the combination and the analysis of historical and modern data in order to highlight in a faster manner the differences between maps dated on various time periods.},
author = {Iosifescu, Ionut and Tsorlini, Angeliki and Hurni, Lorenz},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/Iosifescu, Tsorlini, Hurni - 2016 - Towards a comprehensive methodology for automatic vectorization of raster historical maps.pdf:pdf},
journal = {e-Perimetron},
keywords = {automatic vectorization,historical maps,raster to vector,shape recognition,vectorization algorithms},
number = {2},
pages = {57--76},
title = {{Towards a comprehensive methodology for automatic vectorization of raster historical maps}},
volume = {11},
year = {2016}
}
@article{Sabour2017,
abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation paramters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule. The final version of the paper is under revision to encorporate reviewers comments.},
archivePrefix = {arXiv},
arxivId = {1710.09829},
author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey},
eprint = {1710.09829},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/1710.09829.pdf:pdf},
journal = {Nips},
number = {Nips},
title = {{Dynamic Routing between Capsules}},
url = {https://research.google.com/pubs/pub46351.html},
year = {2017}
}
@article{Dharmaraj2005,
annote = {Ulike algoritmer, kun p{\aa} {\aa} hente ut linjer.},
author = {Dharmaraj, Girija},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/05.20226.Girija-Dharmaraj.pdf:pdf},
number = {20226},
title = {{Algorithms for Automatic Vectorization of Scanned Maps}},
year = {2005}
}
@misc{Amazon2017,
author = {{Amazon Web Services}},
title = {{Amazon EC2 Spot Instances Pricing}},
url = {https://aws.amazon.com/ec2/spot/pricing/},
urldate = {2017-12-19},
year = {2017}
}
@article{Wenyin1996,
author = {Wenyin, Liu and Dori, Dov},
doi = {10.1109/ICPR.1996.547280},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/00547280.pdf:pdf},
isbn = {081867282X},
issn = {10514651},
journal = {Proceedings - International Conference on Pattern Recognition},
keywords = {Eiigitieeritig Drawiiigs Ititerpretatioii,Vectorization,bectorizatioii Evalitation},
pages = {808--812},
title = {{Sparse pixel tracking: A fast vectorization algorithm applied to engineering drawings}},
volume = {3},
year = {1996}
}
@article{Sun2017,
abstract = {The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10x or 100x? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between `enormous data' and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.},
archivePrefix = {arXiv},
arxivId = {1707.02968},
author = {Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
doi = {1707.02968},
eprint = {1707.02968},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/1707.02968.pdf:pdf},
title = {{Revisiting Unreasonable Effectiveness of Data in Deep Learning Era}},
url = {http://arxiv.org/abs/1707.02968},
year = {2017}
}
@misc{Bo2009,
author = {B{\o}, Tore},
title = {{En veileder basert p{\aa} praktiske erfaringer fra Telemark og Vestfold}},
url = {https://kartverket.no/globalassets/plan/digitalisering-av-reguleringsplaner-telemark-vestfold.pdf},
year = {2009}
}
@article{Ning2005,
abstract = {We describe a trainable system for analyzing videos of developing C. elegans embryos. The system automatically detects, segments, and locates cells and nuclei in microscopic images. The system was designed as the central component of a fully automated phenotyping system. The system contains three modules 1) a convolutional network trained to classify each pixel into five categories: cell wall, cytoplasm, nucleus membrane, nucleus, outside medium; 2) an energy-based model, which cleans up the output of the convolutional network by learning local consistency constraints that must be satisfied by label images; 3) a set of elastic models of the embryo at various stages of development that are matched to the label images.},
author = {Ning, Feng and Delhomme, Damien and LeCun, Yann and Piano, Fabio and Bottou, L{\'{e}}on and Barbano, Paolo Emilio},
doi = {10.1109/TIP.2005.852470},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/01495508.pdf:pdf},
isbn = {1057-7149 VO - 14},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Convolutional network,Energy-based model,Image segmentation,Nonlinear filter},
number = {9},
pages = {1360--1371},
pmid = {16190471},
title = {{Toward automatic phenotyping of developing embryos from videos}},
volume = {14},
year = {2005}
}
@book{Patterson2017,
author = {Patterson, Josh and Gibson, Adam},
publisher = {O'Reilly Media},
title = {{Deep Learning: A Practitioner's Approach}},
year = {2017}
}
@misc{Cord,
author = {Cord, Matthieu and Durand, Thibaut and Thome, Nicolas},
title = {{Deep learning and weak supervision for image classification}},
url = {http://thoth.inrialpes.fr/workshop/thoth2016/slides/cord.pdf},
urldate = {2017-11-13}
}
@misc{scan2cad2009,
author = {Scan2cad},
title = {{Scan2CAD in Landscape Architecture}},
url = {https://www.scan2cad.com/user-testimony/scan2cad-in-landscape-architecture/},
urldate = {2017-10-24},
year = {2009}
}
@article{Kazemi2009,
abstract = {This paper review past and current research activity in the area of generalisation of spatial data and presents a new methodological framework for segmentation and generalisation of raster data. In order to overcome drawbacks associated with supervised classification and generalisation of raster data, an Interactive Automated Segmentation and Raster Generalisation Framework (IASRGF) was developed and tested. Test results of the IASGRF shows that all objects derived from the generalisation of landuse data over Canberra, Australia, were well classified and mapped. The error assessment indicates that the percentile classification accuracy is 85.5{\%}, whereas the commission error is relatively high (38.5{\%}). More importantly, the maximum likelihood classifier using training sites and associated ground truth data suggests that the Kappa index is 0.798, which can be interpreted as a reliable and satisfactory classification result. In order to further enhance supervised classification, a post-classification was carried out. As a result, this extra process improved the overall classification accuracy slightly, however its commission error also increased by 6{\%}. {\textcopyright} Geoinformatics International.},
author = {Kazemi, S. and Lim, S. and Rizos, C.},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/Interactive{\_}and{\_}Automated{\_}Segmentation{\_}and{\_}General.pdf:pdf},
issn = {16866576},
journal = {International Journal of Geoinformatics},
number = {3},
pages = {65--73},
title = {{Interactive and Automated segmentation and generalisation of raster data}},
volume = {5},
year = {2009}
}
@article{Russakovsky2015,
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
archivePrefix = {arXiv},
arxivId = {1409.0575},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
eprint = {1409.0575},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/1409.0575.pdf:pdf},
isbn = {0920-5691},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
number = {3},
pages = {211--252},
pmid = {16190471},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
volume = {115},
year = {2015}
}
@article{Zeiler2011,
author = {Zeiler, Matthew D and Taylor, Graham W and Fergus, Rob},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/06126474.pdf:pdf},
isbn = {9781457711022},
pages = {2018--2025},
title = {{Adaptive Deconvolutional Networks for Mid and High Level Feature Learning}},
year = {2011}
}
@article{Chen2016,
abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or 'atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed "DeepLab" system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7{\%} mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
archivePrefix = {arXiv},
arxivId = {1606.00915},
author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
doi = {10.1109/TPAMI.2017.2699184},
eprint = {1606.00915},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/1606.00915.pdf:pdf},
isbn = {0162-8828 VO - PP},
issn = {0162-8828},
pages = {1--14},
pmid = {28463186},
title = {{DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs}},
url = {http://arxiv.org/abs/1606.00915},
year = {2016}
}
@article{Miao2016,
abstract = {Superpixels have been widely used in lots of computer vision and image processing tasks but rarely used in topographic map processing due to the complex distribution of geographic elements in this kind of images. We propose a novel superpixel-generating method based on guided watershed transform (GWT). Before GWT, the cues of geographic element distribution and boundaries between different elements need to be obtained. A linear feature extraction method based on a compound opposite Gaussian filter and a shear transform is presented to acquire the distribution information. Meanwhile, a boundary detection method, which based on the color-opponent mechanisms of the visual system, is employed to get the boundary information. Then, both linear features and boundaries are input to the final partition procedure to obtain superpixels. The experiments show that our method has the best performance in shape control, size control, and boundary adherence, among all the comparison methods, which ar)},
author = {Miao, Qiguang and Liu, Tiange and Song, Jianfeng and Gong, Maoguo and Yang, Yun},
doi = {10.1109/TGRS.2016.2567481},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/07524755.pdf:pdf},
issn = {01962892},
journal = {IEEE Transactions on Geoscience and Remote Sensing},
keywords = {Guided watershed transform (GWT),linear feature extraction,superpixels,topographic map},
number = {11},
pages = {6265--6279},
title = {{Guided Superpixel Method for Topographic Map Processing}},
volume = {54},
year = {2016}
}
@article{Lin2014,
abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
archivePrefix = {arXiv},
arxivId = {1405.0312},
author = {Lin, Tsung Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'{a}}r, Piotr and Zitnick, C. Lawrence},
doi = {10.1007/978-3-319-10602-1_48},
eprint = {1405.0312},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/1405.0312.pdf:pdf},
isbn = {978-3-319-10601-4},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 5},
pages = {740--755},
title = {{Microsoft COCO: Common objects in context}},
volume = {8693 LNCS},
year = {2014}
}
@article{Marmanis2016,
abstract = {See, stats, and : https : / / www . researchgate . net / publication / 307530684 SEMANTIC IMAGES Article DOI : 10 . 5194 / isprsannals - III - 3 - 473 - 2016 CITATION 1 READS 42 6 , including : Some : ProgressTrack - Automated photogrammetric Dimitris German (DLR) 11 SEE Jan ETH 55 SEE Konrad ETH 53 SEE Uwe Technische 454 , 170 SEE All - text , letting. Available : Dimitris Retrieved : 02 ABSTRACT : This paper describes a deep learning approach to semantic segmentation of very high resolution (aerial) images . Deep neural architec - tures hold the promise of end - to - end learning from raw images , making heuristic feature design obsolete . Over the last decade this idea has seen a revival , and in recent years deep convolutional neural networks (CNNs) have emerged as the method of choice for a range of image interpretation tasks like visual recognition and object detection . Still , standard CNNs do not lend themselves to per - pixel semantic segmentation , mainly because one of their fundamental principles is to gradually aggregate information over larger and larger image regions , making it hard to disentangle contributions from different pixels . Very recently two extensions of the CNN framework have made it possible to trace the semantic information back to a precise pixel position : deconvolutional network layers undo the spatial downsampling , and Fully Convolution Networks (FCNs) modify the fully connected classification layers of the network in such a way that the location of individual activations remains explicit . We design a FCN which takes as input intensity and range data and , with the help of aggressive deconvolution and recycling of early network layers , converts them into a pixelwise classification at full resolution . We discuss design choices and intricacies of such a network , and demonstrate that an ensemble of several networks achieves excellent results on challenging data such as the ISPRS semantic labeling benchmark , using only the raw data as input .},
author = {Marmanis, Dimitrios and Wegner, Jan D and Galliani, Silvano and Schindler, K and Datcu, Mihai and Stilla, U},
doi = {10.5194/isprsannals-III-3-473-2016},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/Marmanis{\_}isprs16.pdf:pdf},
issn = {2194-9050},
journal = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, 2016},
pages = {473--480},
title = {{Semantic Segmentation of Aerial Images with an Ensemble of CNSS}},
volume = {3},
year = {2016}
}
@misc{Li,
author = {Karpathy, Andrej},
title = {{CS231n Convolutional Neural Networks for Visual Recognition}},
url = {http://cs231n.github.io/neural-networks-1/},
urldate = {2017-11-10}
}
@article{Armstrong2006,
author = {Armstrong, Curtis A},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/Armstrong - 2006 - Vectorization of Raster Images Using B-Spline Surfaces.pdf:pdf},
keywords = {vector video research preparation,矢量视频调研},
number = {July},
pages = {166},
title = {{Vectorization of Raster Images Using B-Spline Surfaces}},
year = {2006}
}
@article{Papandreou2015,
author = {Papandreou, George},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/07298636.pdf:pdf},
isbn = {9781467369640},
journal = {Cvpr},
pages = {390--399},
title = {{Modeling Local and Global Deformations in Deep Learning {\_} Epitomic Convolution,Multiple Instance Learning, and SlidingWindow Detection}},
year = {2015}
}
@article{Szegedy2014,
abstract = {We propose a deep convolutional neural network ar- chitecture codenamed Inception that achieves the new state of the art for classification and detection in the Im- ageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the compu- tational budget constant. To optimize quality, the architec- tural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular in- carnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew and Hill, Chapel and Arbor, Ann},
doi = {10.1109/CVPR.2015.7298594},
eprint = {1409.4842},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/07298594.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
pages = {1--9},
pmid = {24920543},
title = {{Going Deeper with Convolutions}},
year = {2014}
}
@article{Leyk2010,
abstract = {A novel approach to color image segmentation (CIS) in scanned archival topographic maps of the 19th century is presented. Archival maps provide unique information for GIS-based change detection and are the only spatially contiguous data sources prior to the establishment of remote sensing. Processing such documents is challenging due to their very low graphical quality caused by ageing, manual production and scanning. Typical artifacts are high degrees of mixed and false coloring, as well as blurring in the images. Existing approaches for segmentation in cartographic documents are normally presented using well-conditioned maps. The CIS approach presented here uses information from the local image plane, the frequency domain and color space. As a first step, iterative clustering is based on local homogeneity, frequency of homogeneity-tested pixels and similarity. By defining a peak-finding rule, "hidden" color layer prototypes can be identified without prior knowledge. Based on these prototypes a constrained seeded region growing (SRG) process is carried out to find connected regions of color layers using color similarity and spatial connectivity. The method was tested on map pages with different graphical properties with robust results as derived from an accuracy assessment.},
author = {Leyk, Stefan and Boesch, Ruedi},
doi = {10.1007/s10707-008-0074-z},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/10.1007{\%}2Fs10707-008-0074-z.pdf:pdf},
isbn = {1070700800},
issn = {13846175},
journal = {GeoInformatica},
keywords = {Clustering,Color image segmentation,Constrained seeded region growing,GIS,Local homogeneity,Peak-finding,Topographic maps},
number = {1},
pages = {1--21},
title = {{Colors of the past: Color image segmentation in historical topographic maps based on homogeneity}},
volume = {14},
year = {2010}
}
@misc{OpenVCTeam2017,
author = {{OpenVC Team}},
title = {{OpenCV}},
url = {https://opencv.org/},
urldate = {2017-10-12},
year = {2017}
}
@article{States2005,
author = {States, United},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/US20060041375.pdf:pdf},
isbn = {2005012173},
number = {12},
title = {{AUTOMATED GEOREFERENCING OF DIGITIZED MAP IMAGES}},
volume = {1},
year = {2005}
}
@article{Karabork2008,
abstract = {In the last twenty-five years a great number of vectorization methods were developed. In this paper we first give an overview about the most known methods, and then propose a vectorization algorithm based on Artificial Neural Network method. This algorithm is implemented by using C{\#} programming language. Because distortions in size and location after vectorization are important for mapping applications, we tested our algorithm and software on a cadastral map. We also compared the results of our algorithm with the results of Sparse Pixel Vectorization (SPV) algorithm. Although SPV algorithm delivers better results, our algorithm also givesacceptable results, which are suitable for mapping purposes.},
author = {Karabork, H and Kocer, B. and Bildirici, I O and Yildiz, F. and Aktas, E.},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/Karabork et al. - 2008 - A neural network algorithm for vectorization of 2D maps.pdf:pdf},
journal = {[APRS'08] International Archives of Photogrammetry and Remote Sensing},
keywords = {accuracy,analysis,artificial{\_}intelligence,cad,image,information,spatial information sciences,tracking},
number = {B2},
pages = {473--480},
title = {{A neural network algorithm for vectorization of 2D maps}},
volume = {XXXVII},
year = {2008}
}
@article{Chiang2009,
abstract = {To exploit the road network in raster maps, the first step is to extract the pixels that constitute the roads and then vectorize the road pixels. Identifying colors that represent roads in raster maps for extracting road pixels is difficult since raster maps often contain numerous colors due to the noise introduced during the processes of image compression and scanning. In this paper, we present an approach that minimizes the required user input for identifying the road colors representing the road network in a raster map. We can then use the identified road colors to extract road pixels from the map. Our approach can be used on scanned and compressed maps that are otherwise difficult to process automatically and tedious to process manually. We tested our approach with 100 maps from a variety of sources, which include 90 scanned maps with various compression levels and 10 computer generated maps. We successfully identified the road colors and extracted the road pixels from all test maps with fewer than four user labels per map on average.},
author = {Chiang, Yao Yi and Knoblock, Craig A.},
doi = {10.1109/ICDAR.2009.274},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/b1d2c9c79f308b170899bce12a6cf611a075.pdf:pdf},
isbn = {9780769537252},
issn = {15205363},
journal = {Proceedings of the International Conference on Document Analysis and Recognition, ICDAR},
pages = {838--842},
title = {{A method for automatically extracting road layers from raster maps}},
year = {2009}
}
@misc{GIMP2017,
author = {GIMP},
title = {{GIMP - GNU IMAGE MANIPULATION PROGRAM}},
url = {https://www.gimp.org/},
urldate = {2017-12-16},
year = {2017}
}
@article{Ciresan2012,
abstract = {We address a central problem of neuroanatomy, namely, the automatic segmentation of neuronal structures depicted in stacks of electron microscopy (EM) images. This is necessary to efﬁciently map 3D brain structure and connectivity. To segment biological neuron membranes, we use a special type of deep artiﬁcial neural network as a pixel classiﬁer. The label of each pixel (membrane or nonmembrane) is predicted from raw pixel values in a square window centered on it. The input layer maps each window pixel to a neuron. It is followed by a succession of convolutional and max-pooling layers which preserve 2D information and extract features with increasing levels of abstraction. The output layer produces a calibrated probability for each class. The classiﬁer is trained by plain gradient descent on a 512  512  30 stack with known ground truth, and tested on a stack of the same size (ground truth unknown to the authors) by the organizers of the ISBI 2012 EM Segmentation Challenge. Even without problem-speciﬁc postprocessing, our approach outperforms competing techniques by a large margin in all three considered metrics, i.e. rand error, warping error and pixel error. For pixel error, our approach is the only one outperforming a second human observer.},
annote = {Henter ut linjer fra electron microscopy.},
author = {Ciresan, Dc and Giusti, Alessandro and Gambardella, Lm and Schmidhuber, J},
doi = {10.1.1.300.2221},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/nips2012.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Nips},
pages = {1--9},
title = {{Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images}},
url = {https://papers.nips.cc/paper/4741-deep-neural-networks-segment-neuronal-membranes-in-electron-microscopy-images.pdf},
year = {2012}
}
@article{Lacroix2010,
abstract = {The median-shift, a new clustering algorithm, is proposed to automatically identify the palette of colored graphics, a pre-requisite for graphics vectorization. The median-shift is an iterative process which shifts each data point to the “median” point of its neighborhood defined thanks to a distance measure and a maximum radius, the only parameter of the method. The process is viewed as a graph transformation which converges to a set of clusters made of one or several connected vertices. As the palette identification depends on color perception, the clustering is performed in the L*a*b* feature space. As pixels located on edges are made of mixed colors not expected to be part of the palette, they are removed from the initial data set by an automatic pre-processing. Results are shown on scanned maps and on the Macbeth color chart and compared to well established methods.},
author = {Lacroix, Vinciane},
doi = {10.1007/978-3-642-13728-0_6},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/978-3-642-13728-0{\_}6.pdf:pdf},
isbn = {364213727X},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {clustering,mean-shift,palette extraction},
pages = {61--68},
title = {{Automatic palette identification of colored graphics}},
volume = {6020 LNCS},
year = {2010}
}
@article{He2015,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1007/s11042-017-4440-4},
eprint = {1512.03385},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/1512.03385.pdf:pdf},
isbn = {978-1-4673-6964-0},
issn = {15737721},
journal = {Multimedia Tools and Applications},
keywords = {Convolutional neural networks,Image steganalysis,Residual learning},
pages = {1--17},
pmid = {23554596},
title = {{Deep Residual Learning for Image Recognition}},
year = {2015}
}
@article{Ioffe2015,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1502.03167},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/1502.03167.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
pmid = {15003161},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {http://arxiv.org/abs/1502.03167},
year = {2015}
}
@article{Lin1985,
abstract = {A connection diagram understanding system using a facsimile as its input device has been designed and implemented. The principle of this system is described and examples of its application to some hand-sketched diagrams are shown. In order to reduce processing time, the procedure is started by only accessing the pixels located on the borders of certain meshes to detect characteristic patterns and make a control map. Background judgment and long straight line segment extraction are executed on the control map. Other complicated areas, which are usually a small portion of the whole diagram area, are also indicated by special labels on the map and then processed by a detailed procedure which scans every pixel at these areas. Graph descriptions of diagrams are employed at different steps of the hierarchical understanding. Problems of data compression, diagram retrieval, and diagram editing are discussed. {\textcopyright} 1985.},
author = {Lin, Xinggang and Shimotsuji, Shigeyoshi and Minoh, Michihiko and Sakai, Toshiyuki},
doi = {10.1016/0734-189X(85)90020-9},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/1-s2.0-0734189X85900209-main.pdf:pdf},
isbn = {0734-189X},
issn = {0734189X},
journal = {Computer Vision, Graphics and Image Processing},
number = {1},
pages = {84--106},
title = {{Efficient diagram understanding with characteristic pattern detection}},
volume = {30},
year = {1985}
}
@article{Godfrey2015,
abstract = {Historical maps possess a wealth of information that is, unfortunately, often unreachable by modern mapping applications. While the digitization of historical maps allows the information to be utilized in these applications, it is really, in many cases, the conversion of features (from a raster data type to a vector data type) contained on those maps that enables the information to be used more extensively and easily. This paper highlights the ways the University of Idaho Library enriched a thematic map to become more usable in modern mapping applications. We accomplished this by detailing a work flow within Esri ArcGIS for Desktop that takes advantage of image enhancement, geoprocessing tools, and remote sensing techniques to generate a vector data set from a raster image of a historical map. To a large degree, the general work flow developed can be adapted and utilized on similar thematic maps.},
author = {Godfrey, Bruce and Eveleth, Hayley},
doi = {10.1080/15420353.2014.1001107},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/An Adaptable Approach for Generating Vector Features from Scanned Historical Thematic Maps Using Image Enhancement and Remote Sensing Techniques in a.pdf:pdf},
isbn = {15420353},
issn = {15420361},
journal = {Journal of Map and Geography Libraries},
keywords = {GIS,geographic information systems,geoprocessing,historical maps,raster maps,unsupervised classification,vectorization},
number = {1},
pages = {18--36},
title = {{An adaptable approach for generating vector features from scanned historical thematic maps using image enhancement and remote sensing techniques in a geographic information system}},
volume = {11},
year = {2015}
}
@article{Chen2017a,
abstract = {In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.},
archivePrefix = {arXiv},
arxivId = {1706.05587},
author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
eprint = {1706.05587},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/1706.05587.pdf:pdf},
title = {{Rethinking Atrous Convolution for Semantic Image Segmentation}},
url = {http://arxiv.org/abs/1706.05587},
year = {2017}
}
@article{DeKruger1994,
abstract = {The demand for maps and related cartographic products has increased greatly over the past years. This increase in the demand for cartographic products has greatly increased the work load for the cartographer because current practice requires the cartographer to manually identify and delineate most of the significant cartographic features from an image. The availability of digital image data has made it possible to use the computer to assist in the extraction and delineation of cartographic features. This research presents one approach to automating the delineation of large area features using neural networks for texture pattern classification. ?? 1994.},
annote = {F{\o}rste artikkel om temaet?},
author = {DeKruger, D. and Hunt, B. R.},
doi = {10.1016/0031-3203(94)90030-2},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/1-s2.0-0031320394900302-main.pdf:pdf},
isbn = {00313203 (ISSN)},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Automation of cartography,Extraction of cartographic features,Neural nets},
number = {4},
pages = {461--483},
title = {{Image processing and neural networks for recognition of cartographic area features}},
volume = {27},
year = {1994}
}
@article{Lee2000,
abstract = {Developing an automated vectorizing system as an input method for a geographic information system (GIS) is of extreme importance due to the fact that an input process takes a lot of time and cost in constructing a GIS. Most vectorizing systems require users to set the parameters as appropriately as possible for a particular map image, but it is quite difficult for a novice to adjust the parameters appropriately. This paper proposes a knowledge-based system for automated vectorization, allowing an appropriate choice of the parameters. Since thinning of the input image to produce a skeleton of unit width is a prerequisite for the automated vectorization among several steps, the performance of representative thinning algorithms is systematically evaluated in various map images, and appropriate rules for the maps are devised. Each rule in the knowledge base is characterized by the type of map, and by the resolution, line width, slope and protrusions. Experimental results with various map images show that the proposed system is superior in terms of performance and convenience of use.},
annote = {Kilde p{\aa} hvorfor digitalisering av gamle kart kan v{\ae}re verdifult},
author = {Lee, Kyong Ho and Cho, Sung Bae and Choy, Yoon Chul},
doi = {10.1016/S0952-1976(99)00049-4},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/1-s2.0-S0952197699000494-main.pdf:pdf},
isbn = {8223612712},
issn = {09521976},
journal = {Engineering Applications of Artificial Intelligence},
keywords = {automated vectorization,cartographic maps,geographic information systems,knowledge bases,thinning},
number = {2},
pages = {165--178},
title = {{Automated vectorization of cartographic maps by a knowledge-based system}},
volume = {13},
year = {2000}
}
@misc{OSGeoa,
author = {OSGeo},
title = {{GDAL - Geospatial Data Abstraction Library}},
url = {http://www.gdal.org/},
urldate = {2017-11-01},
year = {2017}
}
@misc{Wu1999,
author = {Wu, Yecheng and {Able Software Corp}},
title = {{R2V}},
url = {http://ablesw.com/r2v/rasvect.html},
urldate = {2017-12-16},
year = {1999}
}
@article{Couclelis1992,
abstract = {A new approach to representing qualitative spatial knowledge and to spatial reasoning is presented. This approach is motivated by cognitive considerations and is based on relative orientation information about spatial environments. The approach aims at exploiting properties of physical space which surface when the spatial knowledge is structured according to conceptual neighborhood of spatial relations. The paper introduces the notion of conceptual neighborhood and its relevance for qualitative temporal reasoning. The extension of the benefits to spatial reasoning is suggested. Several approaches to qualitative spatial reasoning are briefly reviewed. Differences between the temporal and the spatial domain are outlined. A way of transferring a qualitative temporal reasoning method to the spatial domain is proposed. The resulting neighborhood-oriented representation and reasoning approach is presented and illustrated. An example for an application of the approach is discussed.},
author = {Couclelis, Helen},
doi = {10.1007/3-540-55966-3},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/People{\_}Manipulate{\_}Objects{\_}but{\_}Cultivate{\_}Fields{\_}Bey.pdf:pdf},
isbn = {978-3-540-55966-5},
issn = {03029743},
number = {July},
title = {{Theories and Methods of Spatio-Temporal Reasoning in Geographic Space}},
url = {http://link.springer.com/10.1007/3-540-55966-3},
volume = {639},
year = {1992}
}
@article{Habib1999,
author = {Habib, a and Uebbing, R and Asmamaw, a},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/p267-chiang.pdf:pdf},
isbn = {1595931465},
keywords = {Project Report submitted to the Center for Mapping},
pages = {267--276},
title = {{Automatic extraction of road intersections from raster maps}},
year = {1999}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/srivastava14a.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
pmid = {23285570},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@misc{Keras2017,
author = {Keras},
title = {{Keras}},
url = {https://keras.io},
urldate = {2017-12-18},
year = {2017}
}
@article{Garcia-Garcia2017,
abstract = {Image semantic segmentation is more and more being of interest for computer vision and machine learning researchers. Many applications on the rise need accurate and efficient segmentation mechanisms: autonomous driving, indoor navigation, and even virtual or augmented reality systems to name a few. This demand coincides with the rise of deep learning approaches in almost every field or application target related to computer vision, including semantic segmentation or scene understanding. This paper provides a review on deep learning methods for semantic segmentation applied to various application areas. Firstly, we describe the terminology of this field as well as mandatory background concepts. Next, the main datasets and challenges are exposed to help researchers decide which are the ones that best suit their needs and their targets. Then, existing methods are reviewed, highlighting their contributions and their significance in the field. Finally, quantitative results are given for the described methods and the datasets in which they were evaluated, following up with a discussion of the results. At last, we point out a set of promising future works and draw our own conclusions about the state of the art of semantic segmentation using deep learning techniques.},
archivePrefix = {arXiv},
arxivId = {1704.06857},
author = {Garcia-Garcia, Alberto and Orts-Escolano, Sergio and Oprea, Sergiu and Villena-Martinez, Victor and Garcia-Rodriguez, Jose},
doi = {10.1007/978-1-4471-4640-7},
eprint = {1704.06857},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/semantic-instance-review-2017.pdf:pdf},
pages = {1--23},
title = {{A Review on Deep Learning Techniques Applied to Semantic Segmentation}},
url = {http://arxiv.org/abs/1704.06857},
year = {2017}
}
@misc{powertrace2016,
author = {PowerTRACE},
title = {{Taking Corel PowerTRACE for a Test Drive – Knowledge Base}},
url = {https://support.corel.com/hc/en-us/articles/215943948-Taking-Corel-PowerTRACE-for-a-Test-Drive},
urldate = {2017-10-24},
year = {2016}
}
@article{Badrinarayanan2015,
abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN and also with the well known DeepLab-LargeFOV, DeconvNet architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. We show that SegNet provides good performance with competitive inference time and more efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.},
archivePrefix = {arXiv},
arxivId = {1511.00561},
author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
doi = {10.1109/TPAMI.2016.2644615},
eprint = {1511.00561},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/1511.00561.pdf:pdf},
isbn = {9783319464879},
issn = {0162-8828},
pages = {1--14},
pmid = {28060704},
title = {{SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation}},
url = {http://arxiv.org/abs/1511.00561},
year = {2015}
}
@article{Yilmaz2012,
author = {Yilmaz, I. and Gullu, M.},
doi = {10.1111/j.1747-1567.2010.00694.x},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/Yilmaz{\_}et{\_}al-2012-Experimental{\_}Techniques.pdf:pdf},
issn = {07328818},
journal = {Experimental Techniques},
keywords = {Affine Coordinate Transformation,Artificial Neural Network,Cyprus Island,Georeferencing,Historical Map,Piri Reis},
number = {5},
pages = {15--19},
title = {{Georeferencing of historical maps using back propagation artificial neural network}},
volume = {36},
year = {2012}
}
@article{Wenyin1999,
abstract = {Vectorisation of raster line images is a relatively mature subject in the document analysis and recognition field, but it is far from being perfect as yet. We survey the methods and algorithms developed to-date for the vectorisation of document images, and classify them into six categories: Hough transform-based, thinning-based, contour-based, run-graph-based, mesh-pattern-based, and sparse-pixel-based. The purpose of the survey is to provide researchers with a comprehensive overview of this technique, to enable a judicious decision while selecting a vectorisation algorithm for a system under development or a newly developed vectorisation algorithm.},
author = {Wenyin, Liu and Dori, Dov},
doi = {10.1007/s100440050010},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/s100440050010.pdf:pdf},
isbn = {1433-7541},
issn = {1433-7541},
journal = {Pattern Analysis {\&} Applications},
keywords = {document analysis and recognition,line drawings,polygonalisation,raster-to-vector,thinning,vectorisation},
number = {1},
pages = {10--21},
title = {{From Raster to Vectors: Extracting Visual Information from Line Drawings}},
url = {http://link.springer.com/10.1007/s100440050010},
volume = {2},
year = {1999}
}
@article{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
doi = {http://dx.doi.org/10.1016/j.protcy.2014.09.007},
eprint = {1102.0183},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances In Neural Information Processing Systems},
pages = {1--9},
pmid = {7491034},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
@misc{PASCALVOC2012,
author = {{PASCAL VOC}},
title = {{PASCAL VOC2011}},
url = {http://host.robots.ox.ac.uk/pascal/VOC/voc2012/segexamples/index.html},
urldate = {2017-11-08},
year = {2012}
}
@article{Lin2016,
abstract = {Recently, very deep convolutional neural networks (CNNs) have shown outstanding performance in object recognition and have also been the first choice for dense classification problems such as semantic segmentation. However, repeated subsampling operations like pooling or convolution striding in deep CNNs lead to a significant decrease in the initial image resolution. Here, we present RefineNet, a generic multi-path refinement network that explicitly exploits all the information available along the down-sampling process to enable high-resolution prediction using long-range residual connections. In this way, the deeper layers that capture high-level semantic features can be directly refined using fine-grained features from earlier convolutions. The individual components of RefineNet employ residual connections following the identity mapping mindset, which allows for effective end-to-end training. Further, we introduce chained residual pooling, which captures rich background context in an efficient manner. We carry out comprehensive experiments and set new state-of-the-art results on seven public datasets. In particular, we achieve an intersection-over-union score of 83.4 on the challenging PASCAL VOC 2012 dataset, which is the best reported result to date.},
archivePrefix = {arXiv},
arxivId = {1611.06612},
author = {Lin, Guosheng and Milan, Anton and Shen, Chunhua and Reid, Ian},
doi = {10.1109/CVPR.2017.549},
eprint = {1611.06612},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/1611.06612.pdf:pdf},
title = {{RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation}},
url = {http://arxiv.org/abs/1611.06612},
year = {2016}
}
@article{Everingham2010,
abstract = {The PASCAL Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection. This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension.},
author = {Everingham, Mark and {Van Gool}, Luc and Williams, Christopher K.I. and Winn, John and Zisserman, Andrew},
doi = {10.1007/s11263-009-0275-4},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/s11263-009-0275-4.pdf:pdf},
isbn = {0920-5691},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Database,Object detection,Object recognition},
number = {2},
pages = {303--338},
pmid = {20713396},
title = {{The pascal visual object classes (VOC) challenge}},
volume = {88},
year = {2010}
}
@article{Yuan2012,
author = {Yuan, Nicholas Jing and Zheng, Yu and Xie, Xing},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/mapsegmentation.pdf:pdf},
pages = {2--6},
title = {{Segmentation of Urban Areas Using Road Networks}},
url = {http://ofuturescholar.com/paperpage?docid=737984},
year = {2012}
}
@article{Ren2003,
abstract = {We propose a two-class classification model for grouping. Human segmented natural images are used as positive examples. Negative examples of grouping are constructed by randomly matching human segmentations and images. In a preprocessing stage an image is over-segmented into super-pixels. We define a variety of features derived from the classical Gestalt cues, including contour, texture, brightness and good continuation. Information-theoretic analysis is applied to evaluate the power of these grouping cues. We train a linear classifier to combine these features. To demonstrate the power of the classification model, a simple algorithm is used to randomly search for good segmentations. Results are shown on a wide range of images.},
author = {Ren, Xiaofeng and Malik, J.},
doi = {10.1109/ICCV.2003.1238308},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/xren{\_}iccv03{\_}discrim.pdf:pdf},
isbn = {0-7695-1950-4},
issn = {0769519504},
journal = {Proceedings Ninth IEEE International Conference on Computer Vision},
keywords = {classification,segmentation},
number = {c},
pages = {10--17 vol.1},
title = {{Learning a classification model for segmentation}},
url = {http://ieeexplore.ieee.org/xpl/login.jsp?tp={\&}arnumber=1238308{\&}url=http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1238308},
volume = {1},
year = {2003}
}
@article{Dori1997,
abstract = {Vectorization is the process of extracting bars (straight line segments) from a binary image. It is the first processing step in a system for understanding engineering drawings. This work presents the Orthogonal Zig-Zag (OZZ) algorithm as an efficient vectorization method, shows its suitability to vectorization of engineering drawings and compares it to the Hough Transform (HT). The underlying idea of OZZ is inspired by a light beam conducted by an optic fiber: a one-pixel-wide ‘ray' travels through a black pixel area designating a bar, as if it were a conducting pipe. The ray's trajectory is parallel to the drawing axes, and its course zig-zags orthogonally, changing direction by 90° each time a white area is encountered. Accumulated statistics about the two sets of black run-lengths gathered along the way provide data for deciding about the presence of a bar, its endpoints and its width, and enable skipping junctions. Using the object-process analysis methodology, the paper provides an overview of the OZZ algorithm by explicitly showing the algorithm's procedures and the corresponding processes they support. The performance of OZZ is demonstrated on a sample of engineering drawings and compared to HT. The work shows theoretically and empirically that the sparse-pixel approach of OZZ results in about twenty-fold reduction in both space and time complexity compared to HT, while the recognition quality is about 40{\%} higher.},
author = {Dori, Dov},
doi = {10.1016/S0965-9978(96)00035-X},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/1-s2.0-S096599789600035X-main.pdf:pdf},
isbn = {0965-9978},
issn = {09659978},
journal = {Advances in Engineering Software},
keywords = {cad,cam,document analysis,form,hough trans-,line extraction,understanding engineering drawings,vectorization},
number = {1},
pages = {11--24},
title = {{Orthogonal Zig-Zag: An algorithm for vectorizing engineering drawings compared with Hough Transform}},
volume = {28},
year = {1997}
}
@article{Chiang2013,
abstract = {Raster maps are easily accessible and contain rich road information; however, converting the road information to vector format is challenging because of varying image quality, overlapping features, and typical lack of metadata (e.g., map geocoordinates). Previous road vectorization approaches for raster maps typically handle a specific map series and require significant user effort. In this paper, we present a general road vectorization approach that exploits common geometric properties of roads in maps for processing heterogeneous raster maps while requiring minimal user intervention. In our experiments, we compared our approach to a widely used commercial product using 40 raster maps from 11 sources. We showed that overall our approach generated high-quality results with low redundancy with considerably less user input compared with competing approaches.},
annote = {Hough Transform for {\aa} f{\aa} frem ganske gode resultater.},
author = {Chiang, Yao Yi and Knoblock, Craig A.},
doi = {10.1007/s10032-011-0177-1},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/Chiang, Knoblock - 2013 - A general approach for extracting road vector data from raster maps.pdf:pdf},
isbn = {9783642330230},
issn = {14332833},
journal = {International Journal on Document Analysis and Recognition},
keywords = {GIS,Map processing,Raster maps,Road vectorization},
number = {1},
pages = {55--81},
title = {{A general approach for extracting road vector data from raster maps}},
volume = {16},
year = {2013}
}
@article{GiraldoArteaga2013,
abstract = {Polygon and attribute data extraction from historical maps such as US insurance atlases from the 19th and early 20th centuries has so far been a manual task. The New York Public Library (NYPL) currently relies on staff and volunteer work to manually extract polygons and other attribute data from its collection to create new public data sets for the study of urban history. This is a time- intensive task requiring up to several minutes to trace a shapefile and transcribe attributes for a single building. In this paper we propose an approach to automatically extract such attribute data from historical maps. The approach makes use of multiple image processing and statistics utilities to produce desirable results in a fraction of the time required to do by hand. On average, a shapefile for an atlas sheet is generated in {\~{}}11.4 minutes for a total of 23.5 hours of processing time for a whole atlas that contains {\~{}}55,000 polygons; contrast this time frame to NYPL's current manual process that has taken three years to extract about 170,000 polygons across four New York City street atlases. Even with some error rate in the proposed approach, the most cumbersome, time-intensive work (manual polygon drawing) has been reduced to a fraction of its original scope. This new workflow has promising implications for historical GIS.},
author = {{Giraldo Arteaga}, Mauricio},
doi = {10.1145/2534931.2534932},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/p66-arteaga.pdf:pdf},
isbn = {145032536X},
journal = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on MapInteraction},
keywords = {alpha shapes,attribute data extraction,map image tracing},
pages = {66--71},
title = {{Historical map polygon and feature extractor}},
year = {2013}
}
@article{Chiang2013a,
abstract = {Historical maps contain rich cartographic information, such as road networks, but this information is "locked" in images and inaccessible to a geographic information system (GIS). Manual map digitization requires intensive user effort and cannot handle a large number of maps. Previous approaches for automatic map processing generally require expert knowledge in order to fine-tune parameters of the applied graphics recognition techniques and thus are not readily usable for non-expert users. This paper presents an efficient and effective graphics recognition technique that employs interactive user intervention procedures for processing historical raster maps with limited graphical quality. The interactive procedures are performed on color-segmented preprocessing results and are based on straightforward user training processes, which minimize the required user effort for map digitization. This graphics recognition technique eliminates the need for expert users in digitizing map images and provides opportunities to derive unique data for spatiotemporal research by facilitating time-consuming map digitization efforts. The described technique generated accurate road vector data from a historical map image and reduced the time for manual map digitization by 38{\%}.  2013 Springer-Verlag.},
author = {Chiang, Yao Yi and Leyk, Stefan and Knoblock, Craig A.},
doi = {10.1007/978-3-642-36824-0_3},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/chiang12-grec.pdf:pdf},
isbn = {9783642368233},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Color image segmentation,historical raster maps,image cleaning,road vectorization},
pages = {25--35},
title = {{Efficient and robust graphics recognition from historical maps}},
volume = {7423 LNCS},
year = {2013}
}
@article{Schalkoff1989,
author = {Schalkoff, R J},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/Schalkoff - 1989 - Feature Extraction And Shape Classification Of 2-D Polygons Using A Neural Network.pdf:pdf},
pages = {953--958},
title = {{Feature Extraction And Shape Classification Of 2-D Polygons Using A Neural Network}},
year = {1989}
}
@article{Daniil2003,
abstract = {The introduction of modern digital imaging techniques to historical cartography, especially in documenting old map collections, meets the need for extending and applying image processing and relevant photogrammetric tools to the analysis of antique maps in a context in which digital photogrammetry intersects modern theories of digital treatment of old maps. The authors discuss some aspects of controlling the geometric features of the scanned images of antique maps in terms, e.g., of linear, angular, and areal deformation. Collateral effects influencing the scanned map image are also described--namely, the statistics and the image quality of the output map images with respect to the original map and/or the "reference" digital best-scanned image.},
author = {Daniil, M and Tsioukas, V and Papadopoulos, K and Livieratos, E},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/Scanning{\_}options{\_}and{\_}choices{\_}in{\_}digitizing{\_}histori.pdf:pdf},
isbn = {975-561-245-9},
journal = {Proceedings of the XIXth International Symposium, CIPA 2003: new perspectives to save cultural heritage: Antalya (Turkey), 30 September-04 October, 2003},
keywords = {cartography,data processing,digital imaging,maps,photogrammetry},
number = {January},
pages = {99--102},
title = {{Scanning options and choices in digitizing historic maps}},
year = {2003}
}
@article{Achanta2012,
abstract = {Computer vision applications have come to rely increasingly on superpixels in recent years, but it is not always clear what constitutes a good superpixel algorithm. In an effort to understand the benefits and drawbacks of existing methods, we empirically compare five state-of-the-art superpixel algorithms for their ability to adhere to image boundaries, speed, memory efficiency, and their impact on segmentation performance. We then introduce a new superpixel algorithm, simple linear iterative clustering (SLIC), which adapts a k-means clustering approach to efficiently generate superpixels. Despite its simplicity, SLIC adheres to boundaries as well as or better than previous methods. At the same time, it is faster and more memory efficient, improves segmentation performance, and is straightforward to extend to supervoxel generation.},
author = {Achanta, Radhakrishna and Shaji, Appu and Smith, Kevin and Lucchi, Aurelien and Fua, Pascal and S{\"{u}}sstrunk, Sabine},
doi = {10.1109/TPAMI.2012.120},
file = {:C$\backslash$:/Users/Ruben/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Achanta et al. - 2012 - SLIC superpixels compared to state-of-the-art superpixel methods.pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Superpixels,clustering,k-means,segmentation},
number = {11},
pages = {2274--2281},
pmid = {22641706},
title = {{SLIC superpixels compared to state-of-the-art superpixel methods}},
volume = {34},
year = {2012}
}
@article{Simonyan2014a,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
doi = {10.1016/j.infsof.2008.09.005},
eprint = {1409.1556},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/1409.1556.pdf:pdf},
isbn = {0950-5849},
issn = {09505849},
pages = {1--14},
pmid = {16873662},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2014}
}
@article{Ciresan2011,
abstract = {We present a fast, fully parameterizable GPU implementation of Convolutional Neural Network variants. Our feature extractors are neither carefully designed nor pre-wired, but rather learned in a supervised way. Our deep hierarchical architectures achieve the best published results on benchmarks for object classification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with error rates of 2.53{\%}, 19.51{\%}, 0.35{\%}, respectively. Deep nets trained by simple back-propagation perform better than more shallow ones. Learning is surprisingly rapid. NORB is completely trained within five epochs. Test error rates on MNIST drop to 2.42{\%}, 0.97{\%} and 0.48{\%} after 1, 3 and 17 epochs, respectively.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Cireşan, Dan C. and Meier, Ueli and Masci, Jonathan and Gambardella, Luca M. and Schmidhuber, J{\"{u}}rgen},
doi = {10.5591/978-1-57735-516-8/IJCAI11-210},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/210.pdf:pdf},
isbn = {9781577355120},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
pages = {1237--1242},
pmid = {21310177},
title = {{Flexible, high performance convolutional neural networks for image classification}},
year = {2011}
}
@article{Yang2013,
abstract = {Color information plays an important role in better understanding of $\backslash$nnatural scenes by at least facilitating discriminating boundaries of objects or $\backslash$nareas. In this study, we propose a new framework for boundary detection in $\backslash$ncomplex natural scenes based on the color-opponent mechanisms of the visual $\backslash$nsystem. The red-green and blue-yellow color opponent channels in the human $\backslash$nvisual system are regarded as the building blocks for various color perception $\backslash$ntasks such as boundary detection. The proposed framework is a feed forward $\backslash$nhierarchical model, which has direct counterpart to the color-opponent $\backslash$nmechanisms involved in from the retina to the primary visual cortex (V1). $\backslash$nResults show that our simple framework has excellent ability to flexibly capture $\backslash$nboth the structured chromatic and achromatic boundaries in complex scenes.},
author = {Yang, Kaifu and Gao, Shaobing and Li, Chaoyi and Li, Yongjie},
doi = {10.1109/CVPR.2013.362},
file = {:C$\backslash$:/Users/Ruben/Documents/prosjektoppgave/kilder/Yang{\_}Efficient{\_}Color{\_}Boundary{\_}2013{\_}CVPR{\_}paper.pdf:pdf},
isbn = {978-0-7695-4989-7},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer  Vision and Pattern Recognition},
keywords = {biological mechanism,boundary detection,color opponent,natural image,visual system},
pages = {2810--2817},
title = {{Efficient color boundary detection with color-opponent mechanisms}},
year = {2013}
}
